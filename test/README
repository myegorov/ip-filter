In brief, I haven't been able to improve beyond what GCC optimization provides.
I want to have a look into whether cachegrind can provide more insight, but
close to giving up on prefetching (http://valgrind.org/docs/manual/cg-manual.html).

It seems that the CPU "knows better" and all prefetching ends up being
additional overhead. With reference to numbers below: 
we'd want the prefetched version to have fewer misses at the expense of 
more loads, compared to default. In reality, both
loads and misses increase for the prefetched version, which suggests
superfluous loads evicting data from cache.

I've tried disabling GCC optimization (`-O0`) to see the effect, but
that doesn't seem to make a difference, so the optimization seems to be at 
CPU level. Interestingly, if I run the default vs prefetched experiments 
through valgrind (http://valgrind.org/), the prefetched version runs 3x faster. 
I think what's different is that valgrind is simulating the CPU.

Experiment: enter 0.5 million keys into a bloom filter, then query the 
filter for same keys.

Data:

- target number of keys: 500,006
- target false positive probability: 1e-6
- number of hash functions: 20
- bit array size: 14,377,768 bits (1.7 MB)

I've used the GCC's __builtin_prefetch (const void *addr, ...):

https://gcc.gnu.org/onlinedocs/gcc-5.3.0/gcc/Other-Builtins.html

    > This function is used to minimize cache-miss latency by moving data into a
    > cache before it is accessed. You can insert calls to __builtin_prefetch into
    > code for which you know addresses of data in memory that is likely to be
    > accessed soon. If the target supports them, data prefetch instructions are
    > generated. If the prefetch is done early enough before the access then the data
    > will be in the cache by the time it is accessed.


Scenarios tried:

1. no prefetching (default):

algorithm:
  for each ip:
    for each hash function:
      test bitarray[hash]


$ make noprefetch
$ ./noprefetch

...
500006 queries took 0.094916 seconds

$ perf stat -e L1-dcache-load-misses,L1-dcache-loads ./noprefetch

...
Performance counter stats for './noprefetch':

20,747,962      L1-dcache-load-misses:u   #   12.52% of all L1-dcache hits
165,686,262      L1-dcache-loads:u

0.280499022 seconds time elapsed


2. short-look-ahead scenario: prefetch next hash of current ip
  (also tried > 1 next hashes, inserting the __builtin_prefetch at various
  points of the loop).

algorithm:
  for each ip:
    for each hash function:
      (1) test bitarray[hash]
      (2) compute next_hash and prefetch (& bitarray[next_hash]) into L1 cache


$ make prefetch
$ ./prefetch

...
500006 queries took 0.105393 seconds
$ perf stat -e L1-dcache-load-misses,L1-dcache-loads ./simple

...
Performance counter stats for './prefetch':

21,012,068      L1-dcache-load-misses:u   #   11.96% of all L1-dcache hits
175,699,283      L1-dcache-loads:u

0.284112410 seconds time elapsed


3. long-look-ahead scenario: prefetch all k hashes of next ip while 
testing current ip (no prefetching for current ip)

algorithm:
  for each (ip, next_ip) pair:
    (1) for each hash function:
      compute hash(next_ip) and prefetch (& bitarray[hash]) into L1 cache
    (2) for each hash function
      compute hash(ip) & test bitarray[hash]


L1-dcache-load-misses essentially unchanged from default scenario (1, no-prefetching)
L1-dcache-loads increase, as expected.
Also, execution time increases significantly above prefetching scenario (2), because of
all the extra (wasted) hashing computations on next_ip.

4. Do constant hashing work for nothing (must be eliminated as unused by
the compiler, hence the drastic reduction in run time).
Repeatedly look up same index in bitarray, no explicit pre-fetching:

algorithm:
  for each ip:
    compute hash(ip)
    for constant number of iterations:
      look up same index in bit array

There're 1e7 fewer L1-dcache-loads and 1e7 fewer L1-dcache-load-misses.
It's coincidental that we have 0.5e6 entries x 20 hash lookups per entry = 1e7,
because nothing is happening in the innermost loop and the extra loads
seem to be from hash computations.


$ make pseudo
$ ./pseudo

...
500006 queries took 0.007698 seconds
$ perf stat -e L1-dcache-load-misses,L1-dcache-loads ./pseudo


500006 queries took 0.007671 seconds

Performance counter stats for './pseudo':

10,917,399      L1-dcache-load-misses:u   #    7.13% of all L1-dcache hits
153,129,271      L1-dcache-loads:u

0.193881172 seconds time elapsed


5. Do not re-compute hashes. Repeatedly look up same index in bitarray, no explicit
pre-fetching.

algorithm:
  for each ip:
    for constant number of iterations:
      look up same index in bit array

We've now eliminated hash computation for each key & no change in lookups.
The difference in loads compared to case (4) must be due to eliminated 
storing of hashes for each key.


$ make idle
$ perf stat -e L1-dcache-load-misses,L1-dcache-loads ./idle

There're 1e7 fewer L1-dcache-loads and same number of L1-dcache-load-misses
compared to experiment (4).

...
Performance counter stats for './idle':

10,587,779      L1-dcache-load-misses:u   #    7.40% of all L1-dcache hits
143,167,774      L1-dcache-loads:u
0.183422218 seconds time elapsed
